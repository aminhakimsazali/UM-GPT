{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28c29a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please send an email to support@openai.com and include any relevant code you'd like help with.)\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load data from an Excel file into a Pandas DataFrame\n",
    "df = pd.read_excel(\"dataChatGPT.xlsx\")\n",
    "\n",
    "# Convert the DataFrame to a JSON format suitable for API input\n",
    "data = df.to_json(orient=\"records\")\n",
    "\n",
    "# Format the data as JSON\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Replace \"your_api_token\" with your actual API token\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 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\"\n",
    "}\n",
    "\n",
    "# Send a request to the OpenAI GPT API with the data as input\n",
    "response = requests.post(\"https://api.openai.com/v1/engines/davinci/jobs\",\n",
    "                         headers=headers,\n",
    "                         data=json_data)\n",
    "\n",
    "# Print the API response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b8d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please send an email to support@openai.com and include any relevant code you'd like help with.)\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load data from an Excel file into a Pandas DataFrame\n",
    "df = pd.read_excel(\"dataChatGPT.xlsx\")\n",
    "\n",
    "# Convert the DataFrame to a JSON format suitable for API input\n",
    "data = df.to_json(orient=\"records\")\n",
    "\n",
    "# Format the data as JSON\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Replace \"your_api_token\" with your actual API token\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 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\"\n",
    "}\n",
    "\n",
    "# Specify the API endpoint for requesting a summary of the data\n",
    "endpoint = \"https://api.openai.com/v1/engines/davinci/jobs?model=text-davinci-002&prompt=summarize+the+data\"\n",
    "\n",
    "# Send a request to the OpenAI GPT API with the data as input\n",
    "response = requests.post(endpoint, headers=headers, data=json_data)\n",
    "\n",
    "# Print the API response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eabc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0f03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 0.0/70.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.1/70.1 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from openai) (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from aiohttp->openai) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aminh\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "276415ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# openai.api_key = os.getenv(\"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\")\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "start_sequence = \"\\nA:\"\n",
    "restart_sequence = \"\\n\\nQ: \"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \\\"Unknown\\\".\\n\\nQ: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: Unknown\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: How many squigs are in a bonk?\\nA: Unknown\\n\\nQ:\",\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=[\"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2e22e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6ul4udyNfmKoyDszjJkamf6AnHntX at 0x2adff0d60e0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" What is the capital of France?\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678985356,\n",
       "  \"id\": \"cmpl-6ul4udyNfmKoyDszjJkamf6AnHntX\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 7,\n",
       "    \"prompt_tokens\": 223,\n",
       "    \"total_tokens\": 230\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11e18b",
   "metadata": {},
   "source": [
    "## Introduction: Building a Question Answering Bot using Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de14f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import GPT2TokenizerFast\n",
    "from typing import List\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451be2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Marcelo Chierighini of Brazil won the gold medal in the men's high jump at the 2020 Summer Olympics.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Who won the 2020 Summer Olympics men's high jump?\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f3e652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefadadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners of the event.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context:\n",
    "The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.\n",
    "33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places \n",
    "to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).\n",
    "Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following\n",
    "a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance\n",
    "where the athletes of different nations had agreed to share the same medal in the history of Olympics. \n",
    "Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a \n",
    "'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and \n",
    "Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump\n",
    "for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg\n",
    "of Sweden (1984 to 1992).\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9043226",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb44fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3964 rows in the data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th>heading</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Swimming at the 2020 Summer Olympics – Women's 100 metre freestyle</th>\n",
       "      <th>Qualification</th>\n",
       "      <td>The Olympic Qualifying Time for the event is 5...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway at the 2020 Summer Olympics</th>\n",
       "      <th>Road</th>\n",
       "      <td>Norway entered a squad of six riders (four men...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belarus 2020 Summer Olympics scandal</th>\n",
       "      <th>IOC investigation</th>\n",
       "      <td>On 3 August, the IOC launched an investigation...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Archery at the 2020 Summer Olympics – Women's individual</th>\n",
       "      <th>Background</th>\n",
       "      <td>This is the 13th consecutive appearance of the...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monaco at the 2020 Summer Olympics</th>\n",
       "      <th>Swimming</th>\n",
       "      <td>Monaco received a universality invitation from...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                content  \\\n",
       "title                                              heading                                                                \n",
       "Swimming at the 2020 Summer Olympics – Women's ... Qualification      The Olympic Qualifying Time for the event is 5...   \n",
       "Norway at the 2020 Summer Olympics                 Road               Norway entered a squad of six riders (four men...   \n",
       "Belarus 2020 Summer Olympics scandal               IOC investigation  On 3 August, the IOC launched an investigation...   \n",
       "Archery at the 2020 Summer Olympics – Women's i... Background         This is the 13th consecutive appearance of the...   \n",
       "Monaco at the 2020 Summer Olympics                 Swimming           Monaco received a universality invitation from...   \n",
       "\n",
       "                                                                      tokens  \n",
       "title                                              heading                    \n",
       "Swimming at the 2020 Summer Olympics – Women's ... Qualification         104  \n",
       "Norway at the 2020 Summer Olympics                 Road                   72  \n",
       "Belarus 2020 Summer Olympics scandal               IOC investigation     159  \n",
       "Archery at the 2020 Summer Olympics – Women's i... Background            136  \n",
       "Monaco at the 2020 Summer Olympics                 Swimming               48  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://cdn.openai.com/API/examples/data/olympics_sections_text.csv')\n",
    "df = df.set_index([\"title\", \"heading\"])\n",
    "print(f\"{len(df)} rows in the data.\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449f441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"curie\"\n",
    "\n",
    "DOC_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-doc-001\"\n",
    "QUERY_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-query-001\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5078a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import dict, tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a860de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str) -> List[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text)\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def get_doc_embedding(text: str) -> List[float]:\n",
    "    return get_embedding(text, DOC_EMBEDDINGS_MODEL)\n",
    "\n",
    "def get_query_embedding(text: str) -> List[float]:\n",
    "    return get_embedding(text, QUERY_EMBEDDINGS_MODEL)\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], List[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \")) for idx, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba8a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname: str) -> dict[tuple[str, str], List[float]]:\n",
    "    \"\"\"\n",
    "    Read the document embeddings and their keys from a CSV.\n",
    "    \n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a91231a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = load_embeddings(\"https://cdn.openai.com/API/examples/data/olympics_sections_document_embeddings.csv\")\n",
    "\n",
    "# ===== OR, uncomment the below line to recaculate the embeddings from scratch. ========\n",
    "\n",
    "# context_embeddings = compute_doc_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5a7438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020 Summer Olympics', 'Summary') : [0.0037565305829048, -0.0061981128528714, -0.0087078781798481, -0.0071364338509738, -0.0025227521546185]... (1536 entries)\n"
     ]
    }
   ],
   "source": [
    "# An example embedding:\n",
    "example_entry = list(document_embeddings.items())[0]\n",
    "print(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f19be",
   "metadata": {},
   "source": [
    "## Step 2: Find similar document embeddings to the question embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1da0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: List[float], y: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    We could use cosine similarity or dot product to calculate the similarity between vectors.\n",
    "    In practice, we have found it makes little difference. \n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7f18b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[tuple[str, str], np.array]) -> List[tuple[float, tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_query_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "273baf34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4096,) and (1536,) not aligned: 4096 (dim 0) != 1536 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/591194141.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0morder_document_sections_by_query_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Who won the men's high jump?\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/440498897.py\u001b[0m in \u001b[0;36morder_document_sections_by_query_similarity\u001b[1;34m(query, contexts)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mquery_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_query_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     document_similarities = sorted([\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mvector_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     ], reverse=True)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/440498897.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     document_similarities = sorted([\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mvector_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     ], reverse=True)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/1798639880.py\u001b[0m in \u001b[0;36mvector_similarity\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mIn\u001b[0m \u001b[0mpractice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mit\u001b[0m \u001b[0mmakes\u001b[0m \u001b[0mlittle\u001b[0m \u001b[0mdifference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4096,) and (1536,) not aligned: 4096 (dim 0) != 1536 (dim 0)"
     ]
    }
   ],
   "source": [
    "order_document_sections_by_query_similarity(\"Who won the men's high jump?\", document_embeddings)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b01308",
   "metadata": {},
   "source": [
    "## 3. Add relevant document sections to the query prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb971c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5001995819e641f7b430e3820e563d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130d7659244b4b76b355d55b63e01921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be06b665df1e403e857806de961c0bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af40c688b9b4b40b01a4e65fb09de38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Context separator contains 3 tokens'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "separator_len = len(tokenizer.tokenize(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71c85f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index]\n",
    "        \n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52987f0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4096,) and (1536,) not aligned: 4096 (dim 0) != 1536 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/941636622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m prompt = construct_prompt(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"Who won the 2020 Summer Olympics men's high jump?\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/1704323740.py\u001b[0m in \u001b[0;36mconstruct_prompt\u001b[1;34m(question, context_embeddings, df)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mFetch\u001b[0m \u001b[0mrelevant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmost_relevant_document_sections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morder_document_sections_by_query_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mchosen_sections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/440498897.py\u001b[0m in \u001b[0;36morder_document_sections_by_query_similarity\u001b[1;34m(query, contexts)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mquery_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_query_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     document_similarities = sorted([\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mvector_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     ], reverse=True)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/440498897.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     document_similarities = sorted([\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mvector_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     ], reverse=True)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/1798639880.py\u001b[0m in \u001b[0;36mvector_similarity\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mIn\u001b[0m \u001b[0mpractice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mit\u001b[0m \u001b[0mmakes\u001b[0m \u001b[0mlittle\u001b[0m \u001b[0mdifference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4096,) and (1536,) not aligned: 4096 (dim 0) != 1536 (dim 0)"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt(\n",
    "    \"Who won the 2020 Summer Olympics men's high jump?\",\n",
    "    document_embeddings,\n",
    "    df\n",
    ")\n",
    "\n",
    "print(\"===\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a846f2e",
   "metadata": {},
   "source": [
    "## 4. Anwer the user's question based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb687f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "393b74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings: dict[tuple[str, str], np.array],\n",
    "    show_prompt: bool = False\n",
    ") -> str:\n",
    "    prompt = construct_prompt(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d465cda3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4096,) and (1536,) not aligned: 4096 (dim 0) != 1536 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/141107863.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manswer_query_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Who won the 2020 Summer Olympics men's high jump?\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/2264642081.py\u001b[0m in \u001b[0;36manswer_query_with_context\u001b[1;34m(query, df, document_embeddings, show_prompt)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mshow_prompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m ) -> str:\n\u001b[1;32m----> 7\u001b[1;33m     prompt = construct_prompt(\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/1704323740.py\u001b[0m in \u001b[0;36mconstruct_prompt\u001b[1;34m(question, context_embeddings, df)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mFetch\u001b[0m \u001b[0mrelevant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmost_relevant_document_sections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morder_document_sections_by_query_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mchosen_sections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/440498897.py\u001b[0m in \u001b[0;36morder_document_sections_by_query_similarity\u001b[1;34m(query, contexts)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mquery_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_query_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     document_similarities = sorted([\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mvector_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     ], reverse=True)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/440498897.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     document_similarities = sorted([\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mvector_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_embedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     ], reverse=True)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/1798639880.py\u001b[0m in \u001b[0;36mvector_similarity\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mIn\u001b[0m \u001b[0mpractice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mit\u001b[0m \u001b[0mmakes\u001b[0m \u001b[0mlittle\u001b[0m \u001b[0mdifference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4096,) and (1536,) not aligned: 4096 (dim 0) != 1536 (dim 0)"
     ]
    }
   ],
   "source": [
    "answer_query_with_context(\"Who won the 2020 Summer Olympics men's high jump?\", df, document_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
